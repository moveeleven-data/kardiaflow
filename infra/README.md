# KardiaFlow Infrastructure Deployment Guide

This folder contains the infrastructure-as-code (IaC) scripts for deploying and tearing down the KardiaFlow development environment in Azure using Bicep and the Azure CLI.

---

## What It Deploys

KardiaFlow is designed for use with a **Databricks Premium-tier workspace**, enabling support for dashboards and future extensibility.

> **Note:**  
> KardiaFlow does **not** use Unity Catalog or Delta Live Tables (DLT).  
> It relies on Delta Lake, Auto Loader, and Databricks Jobs for portability and low cost.

---

### Resource Summary

| Resource Group                     | Resources Created                                                                                                   | Notes                                                                                     | Cost Guidance                                                                                                   |
|-----------------------------------|---------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------|
| **kardia‑rg‑dev**                 | • `kardia‑dbx` (Databricks workspace)<br>• `kardiaadlsdemo` (ADLS Gen2 account, `lake` container)                   | Created via Bicep                                                                         | • Workspace control-plane is free while clusters are off<br>• Storage billed by usage (LRS hot tier) |
| **kardia‑dbx‑managed** (autogenerated) | • `dbmanagedidentity`<br>• `dbstorage…`<br>• `workers‑vnet`<br>• `workers‑sg`<br>• `unity-catalog-access-connector` | Provisioned automatically by Databricks                                                  | • All resources are free except `dbstorage` (minimal DBFS usage)                                               |
| **NetworkWatcherRG** (autogenerated)   | • `NetworkWatcher_<region>`                                                                                         | Created by Azure when a VNet is provisioned                                              | Free                                                                                                            |

---

## Deploy Instructions

Run these from your local terminal in the project root. Make sure you're logged into the correct Azure subscription.

**1. Load environment variables from .env**

```bash
source infra/.env
az account set --subscription "$SUB"
```

---

**2. Create the Azure resource group**

```bash
az group create --name "$RG" --location eastus
```

---

**3. Deploy infrastructure with Bicep (Databricks + ADLS)**

```bash
az deployment group create \
  --resource-group "$RG" \
  --template-file infra/bicep/deploy_premium.bicep \
  --name "$DEPLOY"
```

Resource creation typically completes in about five minutes.

---

**4. Generate a Databricks Personal Access Token in the UI**

(Settings → Developer → Generate New Token)

Add it to infra/.env as ```DATABRICKS_PAT=...```

---

**5. Authenticate the Databricks CLI (required to write secrets)**

Set DATABRICKS_HOST and DATABRICKS_TOKEN using PAT.

```bash
source infra/deploy/auth.sh
```

NOTE: Secret scopes and storing secrets in them do not incur Databricks billing charges.

---

**6. Ensure/create the service principal, grant RBAC, rotate the secret, and sync to Databricks**

```bash
bash infra/deploy/ensure_sp.sh --rotate
```

NOTE:
ensure_sp.sh is idempotent (it reuses the existing SP by name unless it’s absent
or you explicitly pass --rotate). So you are not accumulating many unused SPs.

Creating an Azure AD application / service principal, assigning it roles
(e.g., “Storage Blob Data Contributor”), and rotating its secret do **not** incur
Azure billing charges.

---

**7. Build and publish kflow wheel (DBFS + Workspace)**

Publishes a versioned wheel and refreshes a stable requirements file at dbfs:/Shared/libs/kflow-requirements.txt.

```bash
bash infra/deploy/build_push_kflow.sh
```

Ensure every task in the job has the stable requirements library:

```json
"libraries": [
  { "requirements": "dbfs:/Shared/libs/kflow-requirements.txt" }
]
```

---

**Execute `/notebooks/99_utilities/bootstrap_dir.ipynb`**

Ensures source and medallion folders exist and seeds sample files.

---

**8. Create (or reset) the full run batch job**

```bash
# Create
databricks jobs create --json @pipelines/jobs/full_run_batch/kardiaflow_full_run_batch.json
```

```bash
# Reset
databricks jobs reset --json @pipelines/jobs/full_run_batch/reset_kardiaflow_full_run_batch.json
```

Once created, you can launch this job from the Databricks Jobs UI at any time. In the UI, select the job, click Run now, and monitor its tasks in real time. You can adjust parameters there — for example, set mode=stream for the Encounters pipeline.

---

**Tear down all provisioned resources**

```bash
./infra/deploy/teardown.sh
```

The teardown script script will:

- Delete the Databricks workspace (automatically removes the managed RG)
- Delete the main resource group (kardia-rg-dev)
- Print a confirmation message

Resources disappear within 2–5 minutes.

---

### When to Build a New Wheel

If you change any code under kflow/, bump the version in pyproject.toml, then republish:

**1. Update the version number in `pyproject.toml`**

`version = "0.2.6"` → `version = "0.2.7"`

**2. Rebuild and re-upload the new wheel:**

```bash
bash infra/deploy/build_push_kflow.sh
```

Jobs pick up the new wheel automatically via the stable requirements file—no changes to job JSON or notebooks.

---

### Dry-Run Deployment

To preview what the deployment will do without actually creating resources:

```bash
az deployment group what-if \
  --resource-group "$RG" \
  --template-file infra/bicep/deploy.bicep \
  --name "$DEPLOY"
```
