# Kardiaflow Infrastructure Deployment Guide

This folder contains infrastructure-as-code scripts to deploy and tear down the Kardiaflow environment in Azure 
using Bicep and the Azure CLI.

### Prerequisites

- **Azure account** with permission to create a resource group
- **Ability to create a Service Principal**
- **Tools (local):**
  - Azure CLI (latest)
  - Databricks CLI (PAT auth)
  - Bash (macOS/Linux/WSL)
  - Python 3.11+ and `pip` — *required only if you run* `infra/deploy/build_push_kflow.sh` *to build the* `kflow` 
    *wheel*  
    *(If you’re on Python 3.10, ensure the `tomli` package is installed.)*
- **Databricks PAT** (Workspace → User Settings → Developer → *Generate new token*)

---

### Infrastructure Configuration (`infra/.env`)

| Var              | Example                                   | Notes                                                         |
|------------------|-------------------------------------------|---------------------------------------------------------------|
| `SUB`            | `00000000-0000-0000-0000-000000000000`    | Azure Subscription ID (used by CLI + RBAC)                    |
| `RG`             | `kardia-rg-dev`                           | Main resource group for Databricks + ADLS                     |
| `DEPLOY`         | `kardiaflow`                              | Bicep deployment name (used by `build_push_kflow.sh` to read outputs) |
| `ADLS`           | `kardiaadlsdemo`                          | ADLS Gen2 account name                                        |
| `CONT`           | `lake`                                    | ADLS container name                                           |
| `WORKSPACE`      | `kardia-dbx`                              | Databricks workspace name                                     |
| `SCOPE`          | `kardia`                                  | Databricks secret scope used by `kflow` and SP creds          |
| `DATABRICKS_PAT` | `dapi<redacted>`                          | Databricks Personal Access Token                              |

---

### Order of Operations (summary)

1. **Deploy Azure** (RG, Databricks, ADLS via Bicep)  
2. **Auth Databricks CLI** (`source infra/deploy/auth.sh`)  
3. **Create/rotate SP + RBAC + secrets** (`bash infra/deploy/ensure_sp.sh --rotate`)  
4. **Build/publish wheel** (`bash infra/deploy/build_push_kflow.sh`)  
5. **Seed data** (`notebooks/99_utilities/bootstrap_dir.ipynb`)  
6. **Create job** (`pipelines/jobs/full_run_batch/kardiaflow_full_run_batch.json`) → **Run now**  
7. **Teardown** (`./infra/deploy/teardown.sh`)

---

## What It Deploys

### Resource Summary

| Resource Group                     | Resources Created                                                                                                   | Notes                                                                                     | Cost Guidance                                                                                                   |
|-----------------------------------|---------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------|
| **kardia‑rg‑dev**                 | • `kardia‑dbx` (Databricks workspace)<br>• `kardiaadlsdemo` (ADLS Gen2 account, `lake` container)                   | Created via Bicep                                                                         | • Workspace control-plane is free while clusters are off<br>• Storage billed by usage (LRS hot tier) |
| **kardia‑dbx‑managed** (autogenerated) | • `dbmanagedidentity`<br>• `dbstorage…`<br>• `workers‑vnet`<br>• `workers‑sg`<br>• `unity-catalog-access-connector` | Provisioned automatically by Databricks                                                  | • All resources are free except `dbstorage` (minimal DBFS usage)                                               |
| **NetworkWatcherRG** (autogenerated)   | • `NetworkWatcher_<region>`                                                                                         | Created by Azure when a VNet is provisioned                                              | Free                                                                                                            |

---

## Deploy Instructions

Run these from your local terminal in the project root. Make sure you're logged into the correct Azure subscription.

**1. Load environment variables from .env**

```bash
source infra/.env
az account set --subscription "$SUB"
```

---

**2. Create the Azure resource group**

```bash
az group create --name "$RG" --location eastus
```

---

**3. Deploy infrastructure with Bicep (Databricks + ADLS)**

```bash
az deployment group create \
  --resource-group "$RG" \
  --template-file infra/bicep/deploy_premium.bicep \
  --name "$DEPLOY"
```

Resource creation typically completes in about five minutes.

---

**4. Generate a Databricks Personal Access Token in the UI**

(Settings → Developer → Generate New Token)

Add it to infra/.env as ```DATABRICKS_PAT=...```

---

**5. Authenticate the Databricks CLI (required to write secrets)**

Set DATABRICKS_HOST and DATABRICKS_TOKEN using PAT.

```bash
source infra/deploy/auth.sh
```

NOTE: Secret scopes and storing secrets in them do not incur Databricks billing charges.

---

**6. Ensure/create the service principal, grant RBAC, rotate the secret, and sync to Databricks**

```bash
bash infra/deploy/ensure_sp.sh --rotate
```

NOTE:
ensure_sp.sh is idempotent (it reuses the existing SP by name unless it’s absent
or you explicitly pass --rotate). So you are not accumulating many unused SPs.

Creating an Azure AD application / service principal, assigning it roles
(e.g., “Storage Blob Data Contributor”), and rotating its secret do not incur
Azure billing charges.

---

**7. Build and publish kflow wheel (DBFS + Workspace)**

Publishes a versioned wheel and refreshes a stable requirements file at dbfs:/Shared/libs/kflow-requirements.txt.

```bash
bash infra/deploy/build_push_kflow.sh
```

Every task in the job should reference the requirements library:

```json
"libraries": [
  { "requirements": "dbfs:/Shared/libs/kflow-requirements.txt" }
]
```

---

**Execute `/notebooks/99_utilities/bootstrap_dir.ipynb`**

Ensures source and medallion folders exist and seeds sample files.

---

**8. Create (or reset) the full run batch job**

```bash
# Create
databricks jobs create --json @pipelines/jobs/full_run_batch/kardiaflow_full_run_batch.json
```

```bash
# Reset
databricks jobs reset --json @pipelines/jobs/full_run_batch/reset_kardiaflow_full_run_batch.json
```

Once created, you can launch this job from the Databricks Jobs UI at any time. In the UI, select the job, click Run now, and monitor its tasks in real time. You can adjust parameters there — for example, set mode=stream for the Encounters pipeline.

---

**Tear down all provisioned resources**

```bash
./infra/deploy/teardown.sh
```

The teardown script will:

- Delete the Databricks workspace (automatically removes the managed RG)
- Delete the main resource group (kardia-rg-dev)
- Print a confirmation message

Resources disappear within 2–5 minutes.

---

### When to Build a New Wheel

If you change any code under kflow/, bump the version in pyproject.toml, then republish:

**1. Update the version number in `pyproject.toml`**

`version = "0.2.6"` → `version = "0.2.7"`

**2. Rebuild and re-upload the new wheel:**

```bash
bash infra/deploy/build_push_kflow.sh
```

Jobs pick up the new wheel automatically via the stable requirements file—no changes to job JSON or notebooks.

---

### Dry-Run Deployment

To preview what the deployment will do without actually creating resources:

```bash
az deployment group what-if \
  --resource-group "$RG" \
  --template-file infra/bicep/deploy.bicep \
  --name "$DEPLOY"
```

---

> **Note:**  
> Kardiaflow does not use Unity Catalog or Delta Live Tables (DLT).  
> It relies on Delta Lake, Auto Loader, and Databricks Jobs for portability and low cost.