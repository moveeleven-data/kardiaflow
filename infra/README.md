# KardiaFlow Infrastructure Deployment Guide

This folder contains the infrastructure-as-code (IaC) scripts for deploying and tearing down the KardiaFlow development environment in Azure using Bicep and the Azure CLI.

---

## What It Deploys

KardiaFlow is designed for use with a **Databricks Premium-tier workspace**, enabling support for dashboards and future extensibility.

> **Note:**  
> KardiaFlow does **not** use Unity Catalog or Delta Live Tables (DLT).  
> It relies on Delta Lake, Auto Loader, and Databricks Jobs for portability and low cost.

---

### Resource Summary

| Resource Group                     | Resources Created                                                                                                   | Notes                                                                                     | Cost Guidance                                                                                                   |
|-----------------------------------|---------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------|
| **kardia‑rg‑dev**                 | • `kardia‑dbx` (Databricks workspace)<br>• `kardiaadlsdemo` (ADLS Gen2 account, `lake` container)                   | Created via Bicep                                                                         | • Workspace control-plane is free while clusters are off<br>• Storage billed by usage (LRS hot tier) |
| **kardia‑dbx‑managed** (autogenerated) | • `dbmanagedidentity`<br>• `dbstorage…`<br>• `workers‑vnet`<br>• `workers‑sg`<br>• `unity-catalog-access-connector` | Provisioned automatically by Databricks                                                  | • All resources are free except `dbstorage` (minimal DBFS usage)                                               |
| **NetworkWatcherRG** (autogenerated)   | • `NetworkWatcher_<region>`                                                                                         | Created by Azure when a VNet is provisioned                                              | Free                                                                                                            |

---

## Deploy Instructions

Run these from your local terminal in the project root. Make sure you're logged into the correct Azure subscription.

**1. Load environment variables from .env**

```bash
source infra/.env
az account set --subscription "$SUB"
```

---

**2. Create the Azure resource group**

```bash
az group create --name "$RG" --location eastus
```

---

**3. Deploy infrastructure with Bicep (Databricks + ADLS)**

```bash
az deployment group create \
  --resource-group "$RG" \
  --template-file infra/bicep/deploy_premium.bicep \
  --name "$DEPLOY"
```

Resource creation typically completes in about five minutes.

---

**4. Generate a Databricks Personal Access Token in the UI**

(Settings → Developer → Generate New Token)

Add it to infra/.env as ```DATABRICKS_PAT=...```

---

**5. Authenticate the Databricks CLI (required to write secrets)**

```bash
# (auth.sh sets DATABRICKS_HOST and DATABRICKS_TOKEN using PAT)
source infra/.env
source infra/deploy/auth.sh
```

NOTE: Secret scopes and storing secrets in them do not incur Databricks billing charges.

---

**6. Ensure/create the service principal, grant RBAC, rotate the secret, and sync to Databricks**

```bash
bash infra/deploy/ensure_sp.sh --rotate
```

NOTE:
ensure_sp.sh is idempotent (it reuses the existing SP by name unless it’s absent
or you explicitly pass --rotate). So you are not accumulating many unused SPs.

Creating an Azure AD application / service principal, assigning it roles
(e.g., “Storage Blob Data Contributor”), and rotating its secret do **not** incur
Azure billing charges.

---

**7. Build kflow wheel and push to DBFS**

This installs the build tool, creates the .whl package from pyproject.toml, and uploads it to DBFS.

```bash
python -m pip install --upgrade build
python -m build
WHL=$(ls dist/kflow-*-py3-none-any.whl | tail -n 1)

databricks fs mkdirs dbfs:/Shared/libs
databricks fs cp "$WHL" dbfs:/Shared/libs/ --overwrite
```

---

**Execute `/notebooks/99_utilities/bootstrap_dir.ipynb`**

Ensures source and medallion folders exist and seeds sample files.

---

**8. Create (or reset) the full run batch job**

```bash
# Create
databricks jobs create --json @pipelines/jobs/kardiaflow_full_run_batch.json
```

```bash
# Reset
databricks jobs reset --json @pipelines/jobs/reset_kardiaflow_full_run_batch.json
```

Once created, you can launch this job from the Databricks Jobs UI at any time. In the UI, select the job, click Run now, and monitor its tasks in real time. You can adjust parameters there — for example, set mode=stream for the Encounters pipeline.

---

**Tear down all provisioned resources**

```bash
./infra/deploy/teardown.sh
```

The teardown script script will:

- Delete the Databricks workspace (automatically removes the managed RG)
- Delete the main resource group (kardia-rg-dev)
- Print a confirmation message

Resources disappear within 2–5 minutes.

---

### To test locally:

**1. Create a Workspace Files folder:**

```bash
dbutils.fs.mkdirs("file:/Workspace/Shared/libs")
```

**2. Copy your wheel from DBFS to Workspace Files (allowed on Shared UC):**

```bash
dbutils.fs.cp(
    "dbfs:/Shared/libs/kflow-0.4.3-py3-none-any.whl",
    "file:/Workspace/Shared/libs/kflow-0.4.3-py3-none-any.whl"
)
```

**3. Install the wheel on the cluster**

Navigate to Cluster → Libraries → Install New → Python Whl → Workspace

Path: Workspace:/Shared/libs/kflow-0.4.3-py3-none-any.whl

Restart cluster.

**When testing locally, comment out `%pip install -q --no-deps --no-index --find-links=/dbfs/Shared/libs 
kflow`**.

---

### When to Build a New Wheel

If you've made changes to the `kflow` package — such as editing any `.py` files inside the `kflow/` directory — 
follow these steps to deploy your updates:

**1. Update the version number in `pyproject.toml`**

`version = "0.2.6"` → `version = "0.2.7"`

**2. Rebuild and re-upload the new wheel:**

```bash
python -m build --wheel
WHL=$(ls -t dist/kflow-*-py3-none-any.whl | head -1)

databricks fs mkdirs dbfs:/Shared/libs
databricks fs cp "$WHL" dbfs:/Shared/libs/ --overwrite
```

This ensures all notebooks and job runs use the latest version and prevents caching issues with `%pip`.

---

### Dry-Run Deployment

To preview what the deployment will do without actually creating resources:

```bash
az deployment group what-if \
  --resource-group "$RG" \
  --template-file infra/bicep/deploy.bicep \
  --name "$DEPLOY"
```
