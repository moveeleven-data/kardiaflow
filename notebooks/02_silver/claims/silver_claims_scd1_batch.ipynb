{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cba3fd4-16a4-483d-9624-26750d37f270",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# silver_claims_scd1_batch.ipynb\n",
    "# SOURCE: `kardia_bronze.bronze_claims` (CDF)\n",
    "# TARGET: `kardia_silver.silver_claims` (SCD1 upsert)\n",
    "# PATTERN: Read changed rows via CDF; MERGE by updating matching records and inserting new ones (SCD1)\n",
    "# TRIGGER: `availableNow` (one-shot incremental batch)\n",
    "\n",
    "# Install kflow from local wheel for use during job execution\n",
    "%pip install -q --no-deps --no-index --find-links=/dbfs/Shared/libs kflow\n",
    "\n",
    "from pyspark.sql import functions as F, Window\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "from kflow.auth_adls import ensure_adls_oauth\n",
    "from kflow.config import bronze_table, CHANGE_TYPES, silver_paths\n",
    "\n",
    "# Configure Spark with ADLS OAuth credentials and return base ABFS path\n",
    "abfss_base = ensure_adls_oauth()\n",
    "\n",
    "# Set catalog to Hive Metastore (required when not using Unity Catalog)\n",
    "spark.sql(\"USE CATALOG hive_metastore\")\n",
    "\n",
    "# Load table paths and names for the Claims dataset\n",
    "S         = silver_paths(\"claims\")\n",
    "SRC_TABLE = bronze_table(\"claims\")\n",
    "TGT_TABLE = S.table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63cde7ff-e054-4b1f-8f57-b38c2b32abf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Ensure Silver DB and Claims table exist\n",
    "#    We define an explicit schema (including _ingest_ts) to decouple Silver from Bronze schema drift\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {S.db}\")\n",
    "\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {TGT_TABLE} (\n",
    "        claim_id                STRING NOT NULL,\n",
    "        patient_id              STRING,\n",
    "        provider_id             STRING,\n",
    "        claim_amount            DOUBLE,\n",
    "        claim_date              DATE,\n",
    "        diagnosis_code          STRING,\n",
    "        procedure_code          STRING,\n",
    "        claim_status            STRING,\n",
    "        claim_type              STRING,\n",
    "        claim_submission_method STRING,\n",
    "        _ingest_ts              TIMESTAMP,\n",
    "        _batch_id               STRING,\n",
    "        _source_file            STRING\n",
    "    ) USING DELTA\n",
    "    LOCATION '{S.path}'\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31674ed9-1250-4a8a-886f-741b99e3fb58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Define upsert logic to apply SCD1 updates to Silver Claims\n",
    "def upsert_to_silver(batch_df, _):\n",
    "    # Retain inserts and updates only\n",
    "    filtered = (\n",
    "        batch_df\n",
    "          .filter(F.col(\"_change_type\").isin(*CHANGE_TYPES))\n",
    "          .filter(F.col(\"ClaimID\").isNotNull())\n",
    "    )\n",
    "\n",
    "    # Standardize column names and types\n",
    "    renamed = (\n",
    "        filtered.select(\n",
    "            F.col(\"ClaimID\").alias(\"claim_id\"),\n",
    "            F.col(\"PatientID\").alias(\"patient_id\"),\n",
    "            F.col(\"ProviderID\").alias(\"provider_id\"),\n",
    "            F.col(\"ClaimAmount\").cast(\"double\").alias(\"claim_amount\"),\n",
    "            F.to_date(\"ClaimDate\").alias(\"claim_date\"),\n",
    "            F.col(\"DiagnosisCode\").alias(\"diagnosis_code\"),\n",
    "            F.col(\"ProcedureCode\").alias(\"procedure_code\"),\n",
    "            F.col(\"ClaimStatus\").alias(\"claim_status\"),\n",
    "            F.col(\"ClaimType\").alias(\"claim_type\"),\n",
    "            F.col(\"ClaimSubmissionMethod\").alias(\"claim_submission_method\"),\n",
    "            F.col(\"_ingest_ts\"),\n",
    "            F.col(\"_batch_id\"),\n",
    "            F.col(\"_source_file\"),\n",
    "            F.col(\"_commit_version\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Retain only the latest version per claim_id in this micro-batch\n",
    "    w_latest = (Window.partitionBy(\"claim_id\")\n",
    "                      .orderBy(F.col(\"_commit_version\").desc(),\n",
    "                               F.col(\"_commit_timestamp\").desc()))\n",
    "\n",
    "    latest = (\n",
    "        renamed\n",
    "          .withColumn(\"rn\", F.row_number().over(w_latest))\n",
    "          .filter(\"rn = 1\")\n",
    "          .drop(\"rn\", \"_commit_version\")\n",
    "    )\n",
    "    \n",
    "    # Upsert into Silver (update existing Claims, insert new ones)\n",
    "    (DeltaTable.forName(spark, TGT_TABLE)\n",
    "               .alias(\"t\")\n",
    "               .merge(latest.alias(\"s\"), \"t.claim_id = s.claim_id\")\n",
    "               .whenMatchedUpdateAll()\n",
    "               .whenNotMatchedInsertAll()\n",
    "               .execute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c8e6def-a545-488d-bf64-c7179ebe80d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Run incremental one-shot stream using CDF and upsert logic\n",
    "(spark.readStream\n",
    "      .format(\"delta\")\n",
    "      .option(\"readChangeFeed\", \"true\")\n",
    "      .table(SRC_TABLE)\n",
    "\n",
    "      .writeStream\n",
    "      .foreachBatch(upsert_to_silver)\n",
    "      .option(\"checkpointLocation\", S.checkpoint)\n",
    "      .trigger(availableNow=True)\n",
    "      .start()\n",
    "      .awaitTermination())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19a3b8cc-1b41-4189-846b-00fc6f880100",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Verify Silver Claims table row count and preview records.\n",
    "df = spark.table(TGT_TABLE)\n",
    "print(f\"Silver Claims row count: {df.count():,}\")\n",
    "display(df.limit(5))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4880559092059772,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_claims_scd1_batch",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
