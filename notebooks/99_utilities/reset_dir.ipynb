{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%md\n",
    "# Kardiaflow reset\n",
    "\n",
    "Drops Delta tables, deletes storage and checkpoints (bronze, silver, raw/source, enriched),\n",
    "removes databases, and clears the Spark catalog cache to provide a clean slate."
   ],
   "id": "ca23fb94727d660f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from kflow.config import bronze_paths, silver_paths\n",
    "from kflow.auth_adls import ensure_adls_oauth\n",
    "\n",
    "# Use the Hive metastore (no Unity Catalog)\n",
    "spark.sql(\"USE CATALOG hive_metastore\")\n",
    "\n",
    "# 1. Configure Spark with ADLS OAuth credentials\n",
    "ensure_adls_oauth()\n",
    "\n",
    "\n",
    "def safe_rm(path: str, description: str) -> None:\n",
    "    \"\"\"Remove a path via dbutils.fs.rm. Log and continue on error.\"\"\"\n",
    "    try:\n",
    "        dbutils.fs.rm(path, recurse=True)\n",
    "        print(f\"Removed {description}: {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  (ignore) failed removing {description} {path}: {e}\")\n",
    "\n",
    "\n",
    "def safe_drop_table(full_name: str) -> None:\n",
    "    \"\"\"Drop a table: try PURGE first, then regular DROP. Clear cache beforehand.\"\"\"\n",
    "    # Clear catalog cache\n",
    "    try:\n",
    "        spark.catalog.clearCache()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(f\"Dropping table {full_name}\")\n",
    "\n",
    "    # Remove data and metadata\n",
    "    try:\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {full_name} PURGE\")\n",
    "        print(f\"Dropped table {full_name} with PURGE\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"  (ignore) PURGE failed for {full_name}: {e}\")\n",
    "\n",
    "    # Fallback: regular DROP\n",
    "    try:\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {full_name}\")\n",
    "        print(f\"Dropped table {full_name} without PURGE\")\n",
    "    except Exception as e2:\n",
    "        print(f\"  (ignore) failed dropping {full_name}: {e2}\")\n",
    "\n",
    "\n",
    "# 2. Drop known tables from Bronze/Silver\n",
    "to_drop = [\n",
    "    \"kardia_bronze.bronze_encounters\",\n",
    "    \"kardia_bronze.bronze_claims\",\n",
    "    \"kardia_bronze.bronze_patients\",\n",
    "    \"kardia_bronze.bronze_providers\",\n",
    "    \"kardia_bronze.bronze_feedback\",\n",
    "    \"kardia_silver.silver_encounters\",\n",
    "    \"kardia_silver.silver_claims\",\n",
    "    \"kardia_silver.silver_patients\",\n",
    "    \"kardia_silver.silver_providers\",\n",
    "    \"kardia_silver.silver_feedback\",\n",
    "    \"kardia_silver.silver_encounters_enriched\"\n",
    "]\n",
    "for full_name in to_drop:\n",
    "    safe_drop_table(full_name)\n",
    "\n",
    "\n",
    "# 3. Remove storage and checkpoints per dataset\n",
    "datasets = (\"encounters\", \"claims\", \"patients\", \"providers\", \"feedback\")\n",
    "for name in datasets:\n",
    "    # Bronze\n",
    "    P = bronze_paths(name)\n",
    "    print(f\"\\nCleaning bronze layer for '{name}':\")\n",
    "    safe_rm(P.bronze, f\"bronze data for {name}\")\n",
    "    safe_rm(P.checkpoint, f\"bronze checkpoint for {name}\")\n",
    "\n",
    "    # Raw / source\n",
    "    print(f\"Cleaning raw/source layer for '{name}':\")\n",
    "    safe_rm(P.raw, f\"raw/source data for {name}\")\n",
    "\n",
    "    # Silver\n",
    "    S = silver_paths(name)\n",
    "    print(f\"Cleaning silver layer for '{name}':\")\n",
    "    safe_rm(S.path, f\"silver data for {name}\")\n",
    "    safe_rm(S.checkpoint, f\"silver checkpoint for {name}\")\n",
    "\n",
    "\n",
    "# 4. Remove enriched target\n",
    "print(\"\\nCleaning enriched target 'encounters_enriched':\")\n",
    "S_enriched = silver_paths(\"encounters_enriched\")\n",
    "safe_rm(S_enriched.path, \"enriched path\")\n",
    "safe_rm(S_enriched.checkpoint, \"enriched checkpoint\")\n",
    "\n",
    "\n",
    "# 5. Drop databases to remove namespaces\n",
    "print(\"\\nDropping databases (if they exist):\")\n",
    "for db in (\"kardia_bronze\", \"kardia_silver\", \"kardia_gold\"):\n",
    "    try:\n",
    "        spark.sql(f\"DROP DATABASE IF EXISTS {db} CASCADE\")\n",
    "        print(f\"Dropped database {db}\")\n",
    "    except Exception as e:\n",
    "        print(f\" (ignore) failed dropping database {db}: {e}\")\n",
    "\n",
    "\n",
    "# 5. Clear Spark catalog cache (current session only)\n",
    "print(\"\\nClearing Spark catalog cache.\")\n",
    "try:\n",
    "    spark.catalog.clearCache()\n",
    "except Exception as e:\n",
    "    print(f\" (ignore) failed clearing cache: {e}\")\n",
    "\n",
    "print(\"\\nReset complete.\")"
   ],
   "id": "4f5fdaf6a4f2956a"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
