{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62c2b870-621d-4bce-b2b1-df01cb54c301",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Kardiaflow - Bronze Claims Auto Loader\n",
    "\n",
    "**Source:** Raw Parquet files in ADLS\n",
    "\n",
    "**Target:** `kardia_bronze.bronze_claims` (CDF enabled)\n",
    "\n",
    "**Trigger:** Incremental batch via Auto Loader; append to Bronze Claims table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66c22721-8b54-44d1-a56e-18f538ea2364",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "from kflow.config import BRONZE_DB, bronze_paths\n",
    "from kflow.etl_utils import add_audit_cols\n",
    "from kflow.notebook_utils import init, show_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e92b94a-8193-4d9e-8173-4cd72040fa20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 1. Initialize environment and load paths**\n",
    "\n",
    "We start by calling `init()` to set up authentication, Spark configs, and database context.  \n",
    "\n",
    "Then we load the Bronze table metadata for the Claims dataset with `bronze_paths(\"claims\")`, which gives us all storage locations (raw, schema, checkpoint, bad records) and the target Delta table name.\n",
    "\n",
    "**Note:** We donâ€™t manually define a schema here. Parquet is a self-describing format, so Auto Loader can infer and persist the schema reliably. We rely on this in Bronze and enforce stricter types when transforming to Silver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ab1e41d-f141-4ae8-91ce-478fe5485e75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "init()\n",
    "\n",
    "P = bronze_paths(\"claims\")\n",
    "BRONZE_TABLE = P.table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "436b2360-2660-4618-a2ae-c5d25bf2f29d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ensure Bronze DB and Claim table exist (Delta and CDF enabled)\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {BRONZE_DB}\")\n",
    "\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {BRONZE_TABLE}\n",
    "    USING DELTA\n",
    "    COMMENT 'Bronze Parquet ingest of claim records.'\n",
    "    LOCATION '{P.bronze}'\n",
    "    TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d2f0e2f-4638-4e05-9ffe-fc6eab522ec5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 2. Incremental ingestion with Auto Loader**\n",
    "\n",
    "Auto Loader ingests new Parquet claim files from ADLS, appending only fresh data and persisting schema history at `P.schema`. Rows missing a valid `ClaimID` are filtered out, audit columns (`_ingest_ts`, `_source_file`, `_batch_id`) are added for traceability, and malformed records are redirected to `P.bad` so ingestion can continue uninterrupted.\n",
    "\n",
    "Parquet embeds its own schema, so no explicit definition is required in Bronze. With `mergeSchema = true`, the table can evolve as new fields appear while remaining consistent with the stored schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ccd7019-4e27-4d05-a90f-b1595faab7dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define an incremental batch pipeline using Auto Loader\n",
    "stream = (\n",
    "    spark.readStream\n",
    "         .format(\"cloudFiles\")\n",
    "         .option(\"cloudFiles.format\", \"parquet\")\n",
    "         .option(\"cloudFiles.includeExistingFiles\", \"true\")\n",
    "         .option(\"cloudFiles.schemaLocation\", P.schema)\n",
    "         .option(\"badRecordsPath\", P.bad)\n",
    "         .load(P.raw)\n",
    "\n",
    "         # Drop any records without a valid primary key\n",
    "         .filter(F.col(\"ClaimID\").isNotNull())\n",
    "         \n",
    "         # Add ingest timestamp, source file, batch ID\n",
    "         .transform(add_audit_cols)\n",
    "         \n",
    "         .writeStream\n",
    "         .option(\"checkpointLocation\", P.checkpoint)\n",
    "         .option(\"mergeSchema\", \"true\")\n",
    "         .trigger(availableNow=True)\n",
    "         .toTable(BRONZE_TABLE)\n",
    ")\n",
    "stream.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c671408-94d3-4455-84b7-1484326100c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 3. Verify Bronze Claims table**\n",
    "\n",
    "Check that the batch completed successfully by inspecting row counts, previewing sample records, and reviewing Delta load history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "622a42d7-59cd-4d82-a246-a4606a6908e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(BRONZE_TABLE)\n",
    "\n",
    "print(f\"Bronze Claims row count: {df.count():,}\")\n",
    "\n",
    "display(df.limit(5))\n",
    "\n",
    "show_history(P.bronze)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4898618290950570,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_claims_autoloader",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
