{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62c2b870-621d-4bce-b2b1-df01cb54c301",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Kardiaflow - Bronze Claims Auto Loader\n",
    "\n",
    "**Source:** Raw Parquet files in ADLS\n",
    "\n",
    "**Target:** `kardia_bronze.bronze_claims` (CDF enabled)\n",
    "\n",
    "**Trigger:** Incremental batch via Auto Loader; append to Bronze Claims table\n",
    "\n",
    "**Description:** The Bronze layer is our raw ingestion zone. Data lands here directly from source files via Auto Loader with minimal transformation. The original schema is preserved as much as\n",
    "possible, audit fields (_ingest_ts, _batch_id, _source_file) are added, and Change Data Feed is enabled for downstream\n",
    "use. The\n",
    "goal is durability and traceability. Bronze is an auditable record of what was received, not yet cleaned or\n",
    "standardized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66c22721-8b54-44d1-a56e-18f538ea2364",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "from kflow.config import BRONZE_DB, bronze_paths\n",
    "from kflow.etl_utils import add_audit_cols\n",
    "from kflow.notebook_utils import init, show_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e92b94a-8193-4d9e-8173-4cd72040fa20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 1. Initialize environment and load paths**\n",
    "\n",
    "Start by calling `init()` to set up authentication, Spark configs, and database context.\n",
    "\n",
    "Then load the Bronze table metadata for the Claims dataset with `bronze_paths(\"claims\")`, which returns all storage\n",
    "locations (raw, schema, checkpoint, bad records) and the target Delta table name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ab1e41d-f141-4ae8-91ce-478fe5485e75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "init()\n",
    "\n",
    "P = bronze_paths(\"claims\")\n",
    "BRONZE_TABLE = P.table\n",
    "\n",
    "# Ensure audit fields like _ingest_ts use UTC\n",
    "spark.sql(\"SET spark.sql.session.timeZone=UTC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "436b2360-2660-4618-a2ae-c5d25bf2f29d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ensure Bronze DB and Claim table exist (Delta and CDF enabled)\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {BRONZE_DB}\")\n",
    "\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {BRONZE_TABLE}\n",
    "    USING DELTA\n",
    "    COMMENT 'Bronze Parquet ingest of claim records.'\n",
    "    LOCATION '{P.bronze}'\n",
    "    TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d2f0e2f-4638-4e05-9ffe-fc6eab522ec5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 2. Incremental ingestion with Auto Loader**\n",
    "\n",
    "Now we use Auto Loader to ingest new Parquet claim files from ADLS, appending only fresh data and persisting schema\n",
    "history at `P.schema`. Audit\n",
    "columns\n",
    "(`_ingest_ts`,\n",
    "`_source_file`,\n",
    " `_batch_id`) are added for traceability. Malformed records are redirected to `P.bad` so ingestion can continue\n",
    " uninterrupted.\n",
    "\n",
    "Rows missing a valid `ClaimID` (PK) are filtered out. `patient_id` and `provider_id` (FKs) may be null\n",
    " in\n",
    "Bronze. Late-arriving or missing dimension references are reconciled in Silver.\n",
    "\n",
    "Parquet embeds its own schema, so no explicit definition is required in Bronze. With `mergeSchema = true`, the table can evolve as new fields appear while remaining consistent with the stored schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ccd7019-4e27-4d05-a90f-b1595faab7dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define an incremental batch pipeline using Auto Loader\n",
    "stream = (\n",
    "    spark.readStream\n",
    "         .format(\"cloudFiles\")\n",
    "         .option(\"cloudFiles.format\", \"parquet\")\n",
    "         .option(\"cloudFiles.includeExistingFiles\", \"true\")\n",
    "         .option(\"cloudFiles.schemaLocation\", P.schema)\n",
    "         .option(\"badRecordsPath\", P.bad)\n",
    "         .load(P.raw)\n",
    "\n",
    "         # Drop any records without valid PK\n",
    "         .filter(F.col(\"ClaimID\").isNotNull()\n",
    "         \n",
    "         # Add ingest timestamp, source file, batch ID\n",
    "         .transform(add_audit_cols)\n",
    "         \n",
    "         .writeStream\n",
    "         .option(\"checkpointLocation\", P.checkpoint)\n",
    "         .option(\"mergeSchema\", \"true\")\n",
    "         .trigger(availableNow=True)\n",
    "         .toTable(BRONZE_TABLE)\n",
    ")\n",
    "stream.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c671408-94d3-4455-84b7-1484326100c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 3. Verify Bronze Claims table**\n",
    "\n",
    "Check that the batch completed successfully by inspecting row counts, previewing sample records, and reviewing Delta load history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "622a42d7-59cd-4d82-a246-a4606a6908e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(BRONZE_TABLE)\n",
    "\n",
    "print(f\"Bronze Claims row count: {df.count():,}\")\n",
    "\n",
    "display(df.limit(5))\n",
    "\n",
    "show_history(P.bronze)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4898618290950570,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_claims_autoloader",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
