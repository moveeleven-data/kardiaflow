{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78db02d7-fa39-4aaa-ad32-2e25e2c20ea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Kardiaflow - Bronze Providers Auto Loader\n",
    "\n",
    "**Source:** Raw TSV files in ADLS\n",
    "\n",
    "**Target:** `kardia_bronze.bronze_providers` (CDF enabled)\n",
    "\n",
    "**Trigger:** Incremental batch via Auto Loader; append to Bronze Providers table\n",
    "\n",
    "**Description:** The Bronze layer is our raw ingestion zone. Data lands here directly from source files via Auto Loader with minimal transformation. The original schema is preserved as much as\n",
    "possible, audit fields (_ingest_ts, _batch_id, _source_file) are added, and Change Data Feed is enabled for downstream\n",
    "use. The\n",
    "goal is durability and traceability. Bronze is an auditable record of what was received, not yet cleaned or\n",
    "standardized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8aab2e47-c57f-43b5-8d68-761d2adab8d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "from kflow.config import BRONZE_DB, bronze_paths\n",
    "from kflow.etl_utils import add_audit_cols\n",
    "from kflow.notebook_utils import init, show_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe6e8000-3a03-4b92-9a7f-f985d1ce176e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 1. Initialize environment and load paths**\n",
    "\n",
    "Start by calling `init()` to set up authentication, Spark configs, and database context.\n",
    "\n",
    "Then load the Bronze table metadata for the Providers dataset with `bronze_paths(\"providers\")`, which returns all\n",
    "storage locations (raw, schema, checkpoint, bad records) and the target Delta table name.\n",
    "\n",
    "*Note: The Spark session timezone is set to UTC so that audit fields like (_ingest_ts) are consistent and\n",
    "comparable across regions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd77faa9-9254-4305-b84d-b0f3c4f7d944",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "init()\n",
    "\n",
    "P = bronze_paths(\"providers\")\n",
    "BRONZE_TABLE = P.table\n",
    "\n",
    "# Ensure audit fields like _ingest_ts use UTC\n",
    "spark.sql(\"SET spark.sql.session.timeZone=UTC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05b135a5-8539-4e4a-bb13-33ad29b166cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 2. Define schema for TSV input**\n",
    "\n",
    "Provider data arrives as TSV files without schema metadata, so we declare the schema explicitly (ProviderID, ProviderSpecialty, ProviderLocation). This provides consistent typing.\n",
    "\n",
    "Audit columns (`_ingest_ts`, `_batch_id`, `_source_file`, `_rescued_data`) are included for traceability and data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "859aa949-bb8e-440a-962c-163a8cf5b6bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "provider_schema = StructType([\n",
    "    StructField(\"ProviderID\",        StringType(), True),\n",
    "    StructField(\"ProviderSpecialty\", StringType(), True),\n",
    "    StructField(\"ProviderLocation\",  StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ca1b112-71bb-4df5-8566-ceee97397bb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {BRONZE_DB}\")\n",
    "\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {BRONZE_TABLE} (\n",
    "        ProviderID        STRING,\n",
    "        ProviderSpecialty STRING,\n",
    "        ProviderLocation  STRING,\n",
    "        _ingest_ts        TIMESTAMP,\n",
    "        _batch_id         STRING,\n",
    "        _source_file      STRING,\n",
    "        _rescued_data     STRING\n",
    "    )\n",
    "    USING DELTA\n",
    "    COMMENT 'Bronze TSV ingest of Provider records.'\n",
    "    LOCATION '{P.bronze}'\n",
    "    TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2423c7ee-38d7-4026-885c-9da0e3e89ba9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 3. Incremental ingestion with Auto Loader**\n",
    "\n",
    "Now we configure Auto Loader to discover and ingest new TSV files from ADLS. Each run appends only new files.\n",
    "\n",
    "TSV input is read with the CSV reader (tab-delimited, with headers). Because TSV lacks an embedded schema, we define it explicitly and persist it at `P.schema` for consistency. With `mergeSchema = true`, Auto Loader can still evolve the table if new columns appear in future files.  \n",
    "\n",
    "Malformed records are captured in `P.bad` and `_rescued_data`, and audit columns (`_ingest_ts`, `_source_file`,\n",
    "`_batch_id`) are added for traceability. This keeps the Bronze layer stable while leaving room for controlled schema\n",
    "growth. Rows missing `ProviderID` are filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6550e6d-105e-479a-82ef-e6c4c3f2fc05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Define an incremental batch pipeline using Auto Loader\n",
    "\n",
    "# Collect all Auto Loader options\n",
    "auto_loader_opts = {\n",
    "    \"cloudFiles.format\": \"csv\",\n",
    "    \"cloudFiles.includeExistingFiles\": \"true\",\n",
    "    \"cloudFiles.schemaLocation\": P.schema,\n",
    "    \"delimiter\": \"\\t\",\n",
    "    \"header\": \"true\",\n",
    "    \"ignoreEmptyLines\": \"true\",\n",
    "    \"badRecordsPath\": P.bad,\n",
    "    \"rescuedDataColumn\": \"_rescued_data\"\n",
    "}\n",
    "\n",
    "stream = (\n",
    "    spark.readStream\n",
    "         .format(\"cloudFiles\")\n",
    "         .options(**auto_loader_opts)\n",
    "         .schema(provider_schema)\n",
    "         .load(P.raw)\n",
    "\n",
    "         # Drop any records without a valid primary key\n",
    "         .filter(F.col(\"ProviderID\").isNotNull())\n",
    "         \n",
    "         # Add ingest timestamp, source file, batch ID\n",
    "         .transform(add_audit_cols)\n",
    "\n",
    "         .writeStream\n",
    "         .option(\"checkpointLocation\", P.checkpoint)\n",
    "         .option(\"mergeSchema\", \"true\")\n",
    "         .trigger(availableNow=True)\n",
    "         .toTable(BRONZE_TABLE)\n",
    ")\n",
    "stream.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a275dd0e-6819-45da-b3b9-fb31c8176e48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 4. Verify Bronze Providers table**\n",
    "\n",
    "Confirm that ingestion succeeded by checking row counts, previewing sample records, and reviewing the Delta load history for this batch.![](path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a4c5497-1358-4652-a7b9-9950baaff3d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(BRONZE_TABLE)\n",
    "\n",
    "print(f\"Bronze Providers row count: {df.count():,}\")\n",
    "\n",
    "display(df.limit(5))\n",
    "\n",
    "show_history(P.bronze)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_providers_autoloader",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
