{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%md\n",
    "### Kardiaflow - Bronze Feedback COPY INTO\n",
    "\n",
    "**Source:** Raw JSON-lines files in ADLS\n",
    "\n",
    "**Target:** `kardia_bronze.bronze_feedback` (CDF enabled)\n",
    "\n",
    "**Trigger:** Incremental batch via COPY INTO; append to Bronze Feedback table\n",
    "\n",
    "Notes:\n",
    "- Feedback arrives in small, asynchronous batches.\n",
    "- COPY INTO is simple and stateless, ideal for sporadic files.\n",
    "- Schema is enforced in the target table; fields are CAST during ingestion."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 0,
   "source": [
    "from pyspark.sql.types import (StructType, StructField, StringType, IntegerType,\n",
    "                               ArrayType, MapType)\n",
    "\n",
    "from kflow.config import BRONZE_DB, bronze_paths, current_batch_id\n",
    "from kflow.notebook_utils import init, show_history\n",
    "\n",
    "# 1. Initialize notebook environment (auth and catalog)\n",
    "init()\n",
    "\n",
    "# Load table paths and names for the Feedback dataset (paths, table, schema, etc.)\n",
    "P            = bronze_paths(\"feedback\")\n",
    "BRONZE_TABLE = P.table\n",
    "BATCH_ID     = current_batch_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a42a8e1d-07fd-4773-a024-cc3c60462a2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Ensure Bronze DB and Feedback table exist\n",
    "# - COPY INTO requires the target Delta table to already exist\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {BRONZE_DB}\")\n",
    "\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {BRONZE_TABLE} (\n",
    "      feedback_id        STRING NOT NULL,\n",
    "      provider_id        STRING,\n",
    "      timestamp          STRING,\n",
    "      visit_id           STRING,\n",
    "      satisfaction_score INT,\n",
    "      comments           STRING,\n",
    "      source             STRING,\n",
    "      tags               ARRAY<STRING>,\n",
    "      metadata_json      STRING,\n",
    "      _ingest_ts         TIMESTAMP,\n",
    "      _source_file       STRING,\n",
    "      _batch_id          STRING\n",
    "    )\n",
    "    USING DELTA\n",
    "    COMMENT 'Bronze JSONL ingest of Feedback records.'\n",
    "    LOCATION '{P.bronze}'\n",
    "    TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07d5e298-7b8c-4304-a198-253d4bce964f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Run batch operation\n",
    "#    COPY INTO scans the entire source path each run\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    COPY INTO {BRONZE_TABLE}\n",
    "    FROM (\n",
    "      SELECT\n",
    "        CAST(feedback_id        AS STRING)            AS feedback_id,\n",
    "        CAST(provider_id        AS STRING)            AS provider_id,\n",
    "        CAST(timestamp          AS STRING)            AS timestamp,\n",
    "        CAST(visit_id           AS STRING)            AS visit_id,\n",
    "        CAST(satisfaction_score AS INT)               AS satisfaction_score,\n",
    "        CAST(comments           AS STRING)            AS comments,\n",
    "        CAST(source             AS STRING)            AS source,\n",
    "        CAST(tags               AS ARRAY<STRING>)     AS tags,\n",
    "        to_json(metadata)                             AS metadata_json,\n",
    "        current_timestamp()                           AS _ingest_ts,\n",
    "        input_file_name()                             AS _source_file,\n",
    "        '{BATCH_ID}'                                  AS _batch_id\n",
    "      FROM '{P.raw}'\n",
    "    )\n",
    "    FILEFORMAT = JSON\n",
    "    FORMAT_OPTIONS ('multiLine' = 'false')\n",
    "    COPY_OPTIONS ('mergeSchema' = 'false')\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d205cf6f-07fc-4a32-954c-345730acfe90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Batch finished. Verify Bronze Feedback table and history\n",
    "df = spark.table(BRONZE_TABLE)\n",
    "print(f\"Bronze Feedback row count: {df.count():,}\")\n",
    "display(df.limit(5))\n",
    "show_history(P.bronze)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_feedback_copy_into",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
