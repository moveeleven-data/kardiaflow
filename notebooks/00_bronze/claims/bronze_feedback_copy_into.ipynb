{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b92916fd-d799-4f23-92f9-853efda55bc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Kardiaflow - Bronze Feedback COPY INTO\n",
    "\n",
    "**Source:** Raw JSON-lines files in ADLS\n",
    "\n",
    "**Target:** `kardia_bronze.bronze_feedback` (CDF enabled)\n",
    "\n",
    "**Trigger:** Incremental batch via COPY INTO; append to Bronze Feedback table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c33f8ce-d883-443f-b965-87dd999f16bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (StructType, StructField, StringType, IntegerType,\n",
    "                               ArrayType, MapType)\n",
    "\n",
    "from kflow.config import BRONZE_DB, bronze_paths, current_batch_id\n",
    "from kflow.notebook_utils import init, show_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25448c5a-b076-4661-9b3b-668c588abefe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 1. Initialize environment and load paths**\n",
    "\n",
    "Start by calling `init()` to set up authentication, Spark configs, and database context.\n",
    "\n",
    "Then load the Bronze table metadata for the Feedback dataset with bronze_paths(\"feedback\"), which returns all storage\n",
    "locations (raw, schema, checkpoint, bad records) and the target Delta table name.\n",
    "\n",
    "*Note: Each run generates a unique BATCH_ID to tag otherwise stateless COPY INTO loads, letting us trace which files were ingested together and track lineage. The Spark session timezone is set to UTC so audit fields like _ingest_ts are consistent and comparable across regions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9db8f10-6945-4899-b702-018bd6028465",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "init()\n",
    "\n",
    "P            = bronze_paths(\"feedback\")\n",
    "BRONZE_TABLE = P.table\n",
    "BATCH_ID     = current_batch_id()\n",
    "\n",
    "# Ensure audit fields like _ingest_ts use UTC\n",
    "spark.sql(\"SET spark.sql.session.timeZone=UTC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6933264f-eb4d-4072-99d4-c227d45b049b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 2. Create Bronze Feedback table**\n",
    "\n",
    "COPY INTO requires the target Delta table to exist ahead of ingestion. We define the Bronze Feedback table with an explicit schema, including both the raw fields from the JSONL source (`feedback_id`, `provider_id`, `timestamp`, etc.) and audit columns (`_ingest_ts`, `_source_file`, `_batch_id`).  \n",
    "\n",
    "Change Data Feed is enabled to allow downstream incremental processing. The schema also normalizes nested structures: for example, `tags` is stored as an array of strings and `metadata` is serialized into `metadata_json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a42a8e1d-07fd-4773-a024-cc3c60462a2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {BRONZE_DB}\")\n",
    "\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {BRONZE_TABLE} (\n",
    "      feedback_id        STRING NOT NULL,\n",
    "      provider_id        STRING NOT NULL,\n",
    "      timestamp          STRING,\n",
    "      visit_id           STRING NOT NULL,\n",
    "      satisfaction_score INT,\n",
    "      comments           STRING,\n",
    "      source             STRING,\n",
    "      tags               ARRAY<STRING>,\n",
    "      metadata_json      STRING,\n",
    "      _ingest_ts         TIMESTAMP,\n",
    "      _source_file       STRING,\n",
    "      _batch_id          STRING\n",
    "    )\n",
    "    USING DELTA\n",
    "    COMMENT 'Bronze JSONL ingest of Feedback records.'\n",
    "    LOCATION '{P.bronze}'\n",
    "    TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e241ca58-7069-4f9d-8a9f-ec03900c9a45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 3. Ingest with COPY INTO**\n",
    "\n",
    "Now we will batch-load new Feedback JSONL files from ADLS using COPY INTO. Each run scans the raw path,\n",
    "skips\n",
    "files already recorded in the tableâ€™s COPY INTO load history, and appends only new ones.\n",
    "\n",
    "Although JSON may evolve, we disable schema evolution (`mergeSchema = false`) in Bronze to keep a stable contract\n",
    ". Fields are explicitly CAST to a known schema to prevent silent type drift, and unexpected keys are captured in\n",
    "`metadata_json` rather than altering the table. Audit columns (`_ingest_ts`, `_source_file`, `_batch_id`) make each\n",
    "otherwise stateless run traceable. Malformed records are redirected to `P.bad` so ingestion can continue\n",
    "uninterrupted. Rows missing valid `feedback_id` (PK), `provider_id` and `visit_id` (FKs) are filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07d5e298-7b8c-4304-a198-253d4bce964f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run batch operation\n",
    "# COPY INTO scans the entire source path each run\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    COPY INTO {BRONZE_TABLE}\n",
    "    FROM (\n",
    "      SELECT\n",
    "        CAST(feedback_id        AS STRING)            AS feedback_id,\n",
    "        CAST(provider_id        AS STRING)            AS provider_id,\n",
    "        CAST(timestamp          AS STRING)            AS timestamp,\n",
    "        CAST(visit_id           AS STRING)            AS visit_id,\n",
    "        CAST(satisfaction_score AS INT)               AS satisfaction_score,\n",
    "        CAST(comments           AS STRING)            AS comments,\n",
    "        CAST(source             AS STRING)            AS source,\n",
    "        CAST(tags               AS ARRAY<STRING>)     AS tags,\n",
    "        to_json(metadata)                             AS metadata_json,\n",
    "        current_timestamp()                           AS _ingest_ts,\n",
    "        input_file_name()                             AS _source_file,\n",
    "        '{BATCH_ID}'                                  AS _batch_id\n",
    "      FROM '{P.raw}'\n",
    "      WHERE feedback_id IS NOT NULL\n",
    "        AND provider_id IS NOT NULL\n",
    "        AND visit_id    IS NOT NULL\n",
    "    )\n",
    "    FILEFORMAT = JSON\n",
    "    FORMAT_OPTIONS ('multiLine' = 'false', 'badRecordsPath' = '{P.bad}')\n",
    "    COPY_OPTIONS ('mergeSchema' = 'false')\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebc13159-5ec8-41c2-a25e-79e962d3090e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 4. Verify Bronze Feedback table**\n",
    "\n",
    "Confirm that ingestion succeeded by checking row counts, previewing sample records, and reviewing the Delta load history for this batch.![](path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d205cf6f-07fc-4a32-954c-345730acfe90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(BRONZE_TABLE)\n",
    "\n",
    "print(f\"Bronze Feedback row count: {df.count():,}\")\n",
    "\n",
    "display(df.limit(5))\n",
    "\n",
    "show_history(P.bronze)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_feedback_copy_into",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
