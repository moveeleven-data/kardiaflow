{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9eb7215d-6a7c-4601-8b4b-03a39d6e1a8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# bronze_feedback_copy_into.ipynb\n",
    "# SOURCE: Raw JSON-lines files in ADLS\n",
    "# TARGET: `kardia_bronze.bronze_feedback` (CDF)\n",
    "# TRIGGER: Incremental batch via COPY INTO; append to Bronze Feedback table\n",
    "# Feedback arrives in small, asynchronous batches - COPY INTO is simple and stateless\n",
    "# Patients and Providers may arrive continuously or in date partitions, making Auto Loader’s checkpointing a better fit\n",
    "\n",
    "from pyspark.sql.types import (StructType, StructField, StringType, IntegerType,\n",
    "                               ArrayType, MapType)\n",
    "\n",
    "from kflow.auth_adls import ensure_adls_oauth\n",
    "from kflow.config import BRONZE_DB, bronze_paths, current_batch_id\n",
    "from kflow.display_utils import show_history\n",
    "\n",
    "# Configure Spark with ADLS OAuth credentials and return base ABFS path\n",
    "abfss_base = ensure_adls_oauth()\n",
    "\n",
    "# Set catalog to Hive Metastore (required when not using Unity Catalog)\n",
    "spark.sql(\"USE CATALOG hive_metastore\")\n",
    "\n",
    "# Load table paths and names for the Feedback dataset (paths, table, schema, etc.)\n",
    "P            = bronze_paths(\"feedback\")\n",
    "BRONZE_TABLE = P.table\n",
    "BATCH_ID     = current_batch_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6eb2dab4-f05f-4006-aaa7-622e8df15f42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define schema explicitly for JSONL input\n",
    "# JSONL doesn’t include schema metadata and inference is unreliable\n",
    "feedback_schema = StructType([\n",
    "    StructField(\"feedback_id\",        StringType(), True),\n",
    "    StructField(\"provider_id\",        StringType(), True),\n",
    "    StructField(\"timestamp\",          StringType(), True),\n",
    "    StructField(\"visit_id\",           StringType(), True),\n",
    "    StructField(\"satisfaction_score\", IntegerType(), True),\n",
    "    StructField(\"comments\",           StringType(), True),\n",
    "    StructField(\"source\",             StringType(), True),\n",
    "    StructField(\"tags\",               ArrayType(StringType()), True),\n",
    "    StructField(\"metadata\",           MapType(StringType(), StringType()), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a42a8e1d-07fd-4773-a024-cc3c60462a2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Ensure Bronze DB and Feedback table exist\n",
    "# - COPY INTO requires the target Delta table to already exist\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {BRONZE_DB}\")\n",
    "\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {BRONZE_TABLE} (\n",
    "      feedback_id        STRING NOT NULL,\n",
    "      provider_id        STRING,\n",
    "      timestamp          STRING,\n",
    "      visit_id           STRING,\n",
    "      satisfaction_score INT,\n",
    "      comments           STRING,\n",
    "      source             STRING,\n",
    "      tags               ARRAY<STRING>,\n",
    "      metadata_json      STRING,\n",
    "      _ingest_ts         TIMESTAMP,\n",
    "      _source_file       STRING,\n",
    "      _batch_id          STRING\n",
    "    )\n",
    "    USING DELTA\n",
    "    COMMENT 'Bronze JSONL ingest of Feedback records.'\n",
    "    LOCATION '{P.bronze}'\n",
    "    TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07d5e298-7b8c-4304-a198-253d4bce964f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Run batch operation\n",
    "#    COPY INTO scans the entire source path each run\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    COPY INTO {BRONZE_TABLE}\n",
    "    FROM (\n",
    "      SELECT\n",
    "        CAST(feedback_id        AS STRING)            AS feedback_id,\n",
    "        CAST(provider_id        AS STRING)            AS provider_id,\n",
    "        CAST(timestamp          AS STRING)            AS timestamp,\n",
    "        CAST(visit_id           AS STRING)            AS visit_id,\n",
    "        CAST(satisfaction_score AS INT)               AS satisfaction_score,\n",
    "        CAST(comments           AS STRING)            AS comments,\n",
    "        CAST(source             AS STRING)            AS source,\n",
    "        CAST(tags               AS ARRAY<STRING>)     AS tags,\n",
    "        to_json(metadata)                             AS metadata_json,\n",
    "        current_timestamp()                           AS _ingest_ts,\n",
    "        input_file_name()                             AS _source_file,\n",
    "        '{BATCH_ID}'                                  AS _batch_id\n",
    "      FROM '{P.raw}'\n",
    "    )\n",
    "    FILEFORMAT = JSON\n",
    "    FORMAT_OPTIONS ('multiLine' = 'false')\n",
    "    COPY_OPTIONS ('mergeSchema' = 'false')\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d205cf6f-07fc-4a32-954c-345730acfe90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Batch finished. Verify Bronze Feedback table and history\n",
    "df = spark.table(BRONZE_TABLE)\n",
    "print(f\"Bronze Feedback row count: {df.count():,}\")\n",
    "display(df.limit(5))\n",
    "show_history(P.bronze)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_feedback_copy_into",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
