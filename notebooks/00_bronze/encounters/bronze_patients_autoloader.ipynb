{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2c9c89f-9933-498b-a2a3-d9152e973c22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# bronze_patients_autoloader.ipynb\n",
    "# SOURCE: Raw CSV files in ADLS\n",
    "# TARGET: `kardia_bronze.bronze_patients` (CDF)\n",
    "# TRIGGER: Incremental batch via Auto Loader; append to Bronze Patients table\n",
    "\n",
    "# Install kflow from local wheel for use during job execution\n",
    "%pip install -q --no-deps --no-index --find-links=/dbfs/Shared/libs kflow\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from kflow.adls import ensure_adls_auth\n",
    "from kflow.config import BRONZE_DB, bronze_paths\n",
    "from kflow.display_utils import show_history\n",
    "from kflow.etl_utils import add_audit_cols\n",
    "\n",
    "# Enable Spark access to ADLS using a SAS token\n",
    "ensure_adls_auth()\n",
    "\n",
    "# Set catalog to Hive Metastore (required when not using Unity Catalog)\n",
    "spark.sql(\"USE CATALOG hive_metastore\")\n",
    "\n",
    "# Load table paths and names for the Patients dataset (paths, table, schema, etc.)\n",
    "P = bronze_paths(\"patients\")\n",
    "BRONZE_TABLE = P.table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b413bc8-9e0d-45c4-9f9f-286653520c90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define schema explicitly for CSV input\n",
    "# CSVs donâ€™t include schema metadata and inference is unreliable\n",
    "patients_schema = StructType([\n",
    "    StructField(\"ID\",         StringType(),  False),\n",
    "    StructField(\"BIRTHDATE\",  StringType(),  True),\n",
    "    StructField(\"DEATHDATE\",  StringType(),  True),\n",
    "    StructField(\"SSN\",        StringType(),  True),\n",
    "    StructField(\"DRIVERS\",    StringType(),  True),\n",
    "    StructField(\"PASSPORT\",   StringType(),  True),\n",
    "    StructField(\"PREFIX\",     StringType(),  True),\n",
    "    StructField(\"FIRST\",      StringType(),  True),\n",
    "    StructField(\"LAST\",       StringType(),  True),\n",
    "    StructField(\"SUFFIX\",     StringType(),  True),\n",
    "    StructField(\"MAIDEN\",     StringType(),  True),\n",
    "    StructField(\"MARITAL\",    StringType(),  True),\n",
    "    StructField(\"RACE\",       StringType(),  True),\n",
    "    StructField(\"ETHNICITY\",  StringType(),  True),\n",
    "    StructField(\"GENDER\",     StringType(),  True),\n",
    "    StructField(\"BIRTHPLACE\", StringType(),  True),\n",
    "    StructField(\"ADDRESS\",    StringType(),  True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2e96d5d-a9d2-43ba-94f7-7d3dd390bce6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Ensure Bronze DB and Patients table exist\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {BRONZE_DB}\")\n",
    "\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {BRONZE_TABLE}\n",
    "    USING DELTA\n",
    "    COMMENT 'Bronze CSV ingest of Patient records.'\n",
    "    LOCATION '{P.bronze}'\n",
    "    TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3a56cdc-896f-44bb-a43b-9af18dbe93df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Define an incremental batch pipeline using Auto Loader\n",
    "stream = (\n",
    "  spark.readStream.format(\"cloudFiles\")\n",
    "       .option(\"cloudFiles.format\", \"csv\")\n",
    "       .option(\"cloudFiles.schemaLocation\", P.schema)\n",
    "       .option(\"cloudFiles.includeExistingFiles\", \"true\")\n",
    "       .option(\"header\", \"true\")\n",
    "       .option(\"ignoreEmptyLines\",\"true\")\n",
    "       .schema(patients_schema)\n",
    "       .load(P.raw)\n",
    "       # Drop any records without a valid primary key\n",
    "       .filter(F.col(\"ID\").isNotNull())\n",
    "       # Add ingest timestamp, source file, batch ID\n",
    "       .transform(add_audit_cols)\n",
    "\n",
    "       .writeStream\n",
    "       .option(\"checkpointLocation\", P.checkpoint)\n",
    "       .option(\"mergeSchema\", \"true\")\n",
    "       .trigger(availableNow=True)\n",
    "       .toTable(BRONZE_TABLE)\n",
    ")\n",
    "stream.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8d983fb-5f9a-4fc6-8b04-559d3496f5b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Batch finished - Verify Bronze Patients table and ingestion history.\n",
    "df = spark.table(BRONZE_TABLE)\n",
    "print(f\"Bronze Patients row count: {df.count():,}\")\n",
    "display(df.limit(5))\n",
    "show_history(P.bronze)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_patients_autoloader",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
