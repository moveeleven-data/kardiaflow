{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "294c7c15-6f65-4f1d-b04f-115eab774d3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Kardiaflow - Bronze Patients Auto Loader\n",
    "\n",
    "**Source:** Raw CSV files in ADLS\n",
    "\n",
    "**Target:** `kardia_bronze.bronze_patients` (CDF enabled)\n",
    "\n",
    "**Trigger:** Incremental batch via Auto Loader; append to Bronze Patients table\n",
    "\n",
    "**Description:** The Bronze layer is our raw ingestion zone. Data lands here directly from source files via Auto Loader with minimal transformation. The original schema is preserved as much as\n",
    "possible, audit fields (_ingest_ts, _batch_id, _source_file) are added, and Change Data Feed is enabled for downstream\n",
    "use. The\n",
    "goal is durability and traceability. Bronze is an auditable record of what was received, not yet cleaned or\n",
    "standardized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6144bad0-8cd1-4004-8400-d3acb09477a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from kflow.config import BRONZE_DB, bronze_paths\n",
    "from kflow.etl_utils import add_audit_cols\n",
    "from kflow.notebook_utils import init, show_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7a09814-0137-4786-91fe-a9df0c1663b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 1. Initialize environment and load paths**\n",
    "\n",
    "Start by calling `init()` to set up authentication, Spark configs, and database context.\n",
    "\n",
    "Then load the Bronze table metadata for the Patients dataset with `bronze_paths(\"patients\")`, which returns all storage\n",
    "locations (raw, schema, checkpoint, bad records) and the target Delta table name.\n",
    "\n",
    "*Note: The Spark session timezone is set to UTC so that audit fields like (_ingest_ts) are consistent and\n",
    "comparable across regions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1808a27a-f3e6-4c7c-bf3e-e3b08eac00d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "init()\n",
    "\n",
    "P = bronze_paths(\"patients\")\n",
    "BRONZE_TABLE = P.table\n",
    "\n",
    "# Ensure audit fields like _ingest_ts use UTC\n",
    "spark.sql(\"SET spark.sql.session.timeZone=UTC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fa24efe-8916-415a-8090-7f7fd03b9ece",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 2. Define schema for CSV input**\n",
    "\n",
    "Patient data arrives as CSV files without schema metadata, so we declare the schema explicitly (`ID`, `BIRTHDATE`, `DEATHDATE`, â€¦ `ADDRESS`). This provides consistent typing across ingestions.  \n",
    "\n",
    "Audit columns (`_ingest_ts`, `_batch_id`, `_source_file`) are added later for traceability and data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b413bc8-9e0d-45c4-9f9f-286653520c90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define schema explicitly for CSV input\n",
    "patients_schema = StructType([\n",
    "    StructField(\"ID\",         StringType(),  False),\n",
    "    StructField(\"BIRTHDATE\",  StringType(),  True),\n",
    "    StructField(\"DEATHDATE\",  StringType(),  True),\n",
    "    StructField(\"SSN\",        StringType(),  True),\n",
    "    StructField(\"DRIVERS\",    StringType(),  True),\n",
    "    StructField(\"PASSPORT\",   StringType(),  True),\n",
    "    StructField(\"PREFIX\",     StringType(),  True),\n",
    "    StructField(\"FIRST\",      StringType(),  True),\n",
    "    StructField(\"LAST\",       StringType(),  True),\n",
    "    StructField(\"SUFFIX\",     StringType(),  True),\n",
    "    StructField(\"MAIDEN\",     StringType(),  True),\n",
    "    StructField(\"MARITAL\",    StringType(),  True),\n",
    "    StructField(\"RACE\",       StringType(),  True),\n",
    "    StructField(\"ETHNICITY\",  StringType(),  True),\n",
    "    StructField(\"GENDER\",     StringType(),  True),\n",
    "    StructField(\"BIRTHPLACE\", StringType(),  True),\n",
    "    StructField(\"ADDRESS\",    StringType(),  True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2e96d5d-a9d2-43ba-94f7-7d3dd390bce6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ensure Bronze DB and Patients table exist\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {BRONZE_DB}\")\n",
    "\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {BRONZE_TABLE}\n",
    "    USING DELTA\n",
    "    COMMENT 'Bronze CSV ingest of Patient records.'\n",
    "    LOCATION '{P.bronze}'\n",
    "    TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6351e24-7a0c-46b2-84ad-234defa58c4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 3. Incremental ingestion with Auto Loader**\n",
    "\n",
    "Now we use Auto Loader to monitor the Patients raw folder in ADLS and ingests new CSV files as they arrive. Each run\n",
    "processes only fresh data, with checkpointing and the schema log (`P.schema`) ensuring files are tracked without reprocessing.\n",
    "\n",
    "CSV has no embedded schema, so we apply the `patients_schema` defined earlier to keep columns and types consistent. With `mergeSchema = true`, the Bronze table can adapt if future files introduce additional fields.  \n",
    "\n",
    "Records missing a valid `ID` (PK) are dropped, and audit fields (`_ingest_ts`, `_source_file`, `_batch_id`) are added\n",
    " for\n",
    " traceability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3a56cdc-896f-44bb-a43b-9af18dbe93df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define an incremental batch pipeline using Auto Loader\n",
    "stream = (\n",
    "  spark.readStream.format(\"cloudFiles\")\n",
    "       .option(\"cloudFiles.format\", \"csv\")\n",
    "       .option(\"cloudFiles.schemaLocation\", P.schema)\n",
    "       .option(\"cloudFiles.includeExistingFiles\", \"true\")\n",
    "       .option(\"header\", \"true\")\n",
    "       .option(\"ignoreEmptyLines\",\"true\")\n",
    "       .schema(patients_schema)\n",
    "       .load(P.raw)\n",
    "\n",
    "       # Drop any records without a valid primary key\n",
    "       .filter(F.col(\"ID\").isNotNull())\n",
    "\n",
    "       # Add ingest timestamp, source file, batch ID\n",
    "       .transform(add_audit_cols)\n",
    "\n",
    "       .writeStream\n",
    "       .option(\"checkpointLocation\", P.checkpoint)\n",
    "       .option(\"mergeSchema\", \"true\")\n",
    "       .trigger(availableNow=True)\n",
    "       .toTable(BRONZE_TABLE)\n",
    ")\n",
    "stream.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df60fa34-7362-4c41-afa2-50fa4a1d6da6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 4. Verify Bronze Patients table**\n",
    "\n",
    "Confirm that ingestion completed successfully by printing the row count, previewing a sample of records, and reviewing the Delta transaction history for the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8d983fb-5f9a-4fc6-8b04-559d3496f5b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(BRONZE_TABLE)\n",
    "\n",
    "print(f\"Bronze Patients row count: {df.count():,}\")\n",
    "\n",
    "display(df.limit(5))\n",
    "\n",
    "show_history(P.bronze)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_patients_autoloader",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
