{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1244cb3c-6be2-47ff-8116-024153ff3aaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Kardiaflow - Bronze Encounters Auto Loader\n",
    "\n",
    "**Source:** Raw Avro files in ADLS\n",
    "\n",
    "**Target:** `kardia_bronze.bronze_encounters` (CDF enabled)\n",
    "\n",
    "**Trigger:** (configurable via job param `mode`)\n",
    "  - `batch` → one-time load of all files\n",
    "  - `stream` → continuous 30s micro-batches\n",
    "\n",
    "**Description:** The Bronze layer is our raw ingestion zone. Data lands here directly from source files via Auto Loader with minimal transformation. The original schema is preserved as much as\n",
    "possible, audit fields (_ingest_ts, _batch_id, _source_file) are added, and Change Data Feed is enabled for downstream\n",
    "use. The\n",
    "goal is durability and traceability. Bronze is an auditable record of what was received, not yet cleaned or\n",
    "standardized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5167863-3201-4c65-9cd0-371671fab4a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "from kflow.config import BRONZE_DB, bronze_paths\n",
    "from kflow.etl_utils import add_audit_cols\n",
    "from kflow.notebook_utils import init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad9f0d5e-58f1-4657-b719-40458500fe74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 1. Initialize environment and load paths**\n",
    "\n",
    "Start by calling `init()` to set up authentication, Spark configs, and database context.\n",
    "\n",
    "The Encounters Bronze metadata is loaded with `bronze_paths(\"encounters\")`, which provides storage locations (raw, schema, checkpoint, bad records) and the target Delta table.\n",
    "\n",
    "*Note: The Spark session timezone is set to UTC so that audit fields like (_ingest_ts) are consistent and\n",
    "comparable across regions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "827ba3a8-e4e3-4d85-a7da-38fc31fad3b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "init()\n",
    "\n",
    "P = bronze_paths(\"encounters\")\n",
    "BRONZE_TABLE = P.table\n",
    "\n",
    "# Ensure audit fields like _ingest_ts use UTC\n",
    "spark.sql(\"SET spark.sql.session.timeZone=UTC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83e72e47-7114-4ecf-a634-26ab87818923",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 2. Retrieve runtime mode**\n",
    "\n",
    "The job accepts a parameter `mode` that controls ingestion:  \n",
    "- **batch**  - process all available files once and stop.  \n",
    "- **stream** - run continuous 30-second micro-batches.  \n",
    "\n",
    "Each mode writes to a separate checkpoint directory so their states remain isolated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdacb534-f643-4cde-addc-1e48e308eea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve runtime mode from job widget (\"batch\" = default, or \"stream\")\n",
    "try:\n",
    "    dbutils.widgets.dropdown(\"mode\", \"batch\", [\"batch\", \"stream\"])\n",
    "except:\n",
    "    # Widget may not exist in interactive mode\n",
    "    pass\n",
    "\n",
    "MODE = dbutils.widgets.get(\"mode\") if \"dbutils\" in globals() else \"batch\"\n",
    "IS_BATCH = (MODE == \"batch\")\n",
    "\n",
    "# Keep batch and stream checkpoints isolated\n",
    "CHECKPOINT = f\"{P.checkpoint}/{MODE}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e01960e-50c4-45cd-a4b8-ba1b90003029",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 3. Create Bronze Encounters table**\n",
    "\n",
    "Now we create the Bronze Encounters table if it doesn’t exist and enable Change Data Feed for downstream incremental\n",
    "processing.\n",
    "\n",
    "Avro is self-describing, so no explicit schema is defined in Bronze. Types are enforced later in Silver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c0259c7-a143-4079-ad8c-8fcf639121bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {BRONZE_DB}\")\n",
    "\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {BRONZE_TABLE}\n",
    "    USING DELTA\n",
    "    COMMENT 'Bronze Avro ingest of Encounter records.'\n",
    "    LOCATION '{P.bronze}'\n",
    "    TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "994d9354-9bc9-4756-b460-a86e4e954ed8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 4. Define Auto Loader pipeline**\n",
    "\n",
    "Now we use Auto Loader to ingest Avro files from the raw path and tracks schema evolution at `P.schema`.\n",
    "\n",
    "Audit fields (`_ingest_ts`, `_source_file`, `_batch_id`)\n",
    " are added for lineage and traceability. The stream is configured with `mergeSchema = true` to accommodate new fields introduced in future files.\n",
    "\n",
    "Rows missing `ID` (PK) are filtered out. `PATIENT` (FK) may be null in Bronze. Late-arriving or\n",
    "missing\n",
    "dimension references are reconciled in Silver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6aefc57-ee55-433a-89e7-2d2945f63119",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define a streaming pipeline using Auto Loader\n",
    "reader = (\n",
    "    spark.readStream.format(\"cloudFiles\")\n",
    "         .option(\"cloudFiles.format\", \"avro\")\n",
    "         .option(\"cloudFiles.schemaLocation\", P.schema)\n",
    "         .option(\"cloudFiles.includeExistingFiles\", \"true\")\n",
    "         .option(\"badRecordsPath\", P.bad)\n",
    "         .load(P.raw)\n",
    "\n",
    "         # Drop any records missing PK\n",
    "         .filter(\n",
    "             F.col(\"ID\").isNotNull()\n",
    "         )\n",
    "\n",
    "         # Add ingest timestamp, source file, batch ID\n",
    "         .transform(add_audit_cols)\n",
    ")\n",
    "\n",
    "writer = (\n",
    "    reader.writeStream\n",
    "          .option(\"checkpointLocation\", CHECKPOINT)\n",
    "          .option(\"mergeSchema\", \"true\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17ba67c2-723d-4bb6-af9b-91207c1e2659",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Step 5. Run batch or stream**\n",
    "\n",
    "Depending on the job parameter, the pipeline executes in one of two modes:  \n",
    "- **Batch mode**: triggers with `availableNow`, ingests all current files, then exits.  \n",
    "- **Stream mode**: runs continuously, processing new data in 30-second intervals.  \n",
    "\n",
    "This flexibility allows the same notebook to support both backfills and live ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e07b844a-7e40-493a-bc6e-28247553eaf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if IS_BATCH:\n",
    "    # Batch mode: process all available files once and exit\n",
    "    query = writer.trigger(availableNow=True).toTable(BRONZE_TABLE)\n",
    "    print(f\"[batch] Wrote to {BRONZE_TABLE} with checkpoint={CHECKPOINT} …\")\n",
    "    query.awaitTermination()\n",
    "else:\n",
    "    # Streaming mode: run continuously every 30s\n",
    "    query = writer.trigger(processingTime=\"30 seconds\").toTable(BRONZE_TABLE)\n",
    "    print(f\"[live] Continuous 30s micro-batches to {BRONZE_TABLE} with checkpoint={CHECKPOINT}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6336542656291775,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_encounters_autoloader",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
