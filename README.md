# Kardiaflow: Azure-Based Healthcare Data Platform

## Scenario

Kardiaflow simulates a real-world healthcare data platform built on Azure Databricks and Delta Lake. It demonstrates a modular, streaming-capable ETL architecture that handles structured (CSV, Avro, TSV) and semi-structured (JSON) healthcare datasets using a medallion design pattern.

The pipeline ingests raw files into Bronze Delta tables using Auto Loader, applies data masking and CDC logic in the Silver layer with Delta Change Data Feed (CDF), and materializes analytics-ready Gold views for reporting and dashboards.



## Architecture Overview

The following diagram illustrates the end-to-end data flow, including ingestion, transformation, validation, and storage layers:

![Kardiaflow Architecture](https://raw.githubusercontent.com/okv627/Kardiaflow/master/docs/assets/kflow_lineage.png?v=2)


## Key Features

**Multi-Domain Simulation**  
&nbsp;&nbsp;&nbsp;&nbsp;â€¢ *Clinical*: Patients, Encounters  
&nbsp;&nbsp;&nbsp;&nbsp;â€¢ *Billing & Feedback*: Claims, Providers, Feedback

**Multi-Format Ingestion**  
&nbsp;&nbsp;&nbsp;&nbsp;â€¢ Structured formats (CSV, Avro, Parquet, TSV) via Auto Loader  
&nbsp;&nbsp;&nbsp;&nbsp;â€¢ Semi-structured JSONL via COPY INTO  
&nbsp;&nbsp;&nbsp;&nbsp;â€¢ All Bronze tables include `_ingest_ts`, `_source_file`, and enable Change Data Feed (CDF)  


**Privacy-Aware Transformations**  
&nbsp;&nbsp;&nbsp;&nbsp;â€¢ Deduplication, PHI masking, SCD Type 1/2  
&nbsp;&nbsp;&nbsp;&nbsp;â€¢ Supports streaming and batch upserts  


**Business-Ready Gold KPIs**  
&nbsp;&nbsp;&nbsp;&nbsp;â€¢ Lifecycle metrics, spend trends, claim anomalies, feedback sentiment  
&nbsp;&nbsp;&nbsp;&nbsp;â€¢ Materializes curated tables for analytics  


**Automated Data Validation**  
&nbsp;&nbsp;&nbsp;&nbsp;â€¢ `99_smoke_checks.py` tests row counts, nulls, duplicates, and schema contracts  
&nbsp;&nbsp;&nbsp;&nbsp;â€¢ Logs results to Delta for auditing and observability  


**Modular Notebook Design**  
&nbsp;&nbsp;&nbsp;&nbsp;â€¢ One notebook per dataset and medallion layer  
&nbsp;&nbsp;&nbsp;&nbsp;â€¢ Clean flow: Raw â†’ Bronze â†’ Silver â†’ Gold  


**Reproducible Infrastructure-as-Code**  
&nbsp;&nbsp;&nbsp;&nbsp;â€¢ Declarative Bicep deployments via Azure CLI  
&nbsp;&nbsp;&nbsp;&nbsp;â€¢ Secrets managed via Databricks CLI  
&nbsp;&nbsp;&nbsp;&nbsp;â€¢ One-command teardown: `infra/teardown.sh`



## Setting Up the Infrastructure

Deploy the full Azure environment via:

[`infra/README.md`](infra/README.md) â€” *Infrastructure Deployment Guide*

> âš ï¸ Commands must be run from the **project root**.

---

## Databricks Summit 2025: How It Shaped Kardiaflow

In June 2025, I completed 24 hours of hands-on training across six advanced data engineering workshops at the 
Databricks Data + AI Summit. These sessions directly influenced Kardiaflowâ€™s design especially in areas like streaming pipeline robustness, CDC with Lakeflow Declarative Pipelines, data governance via Unity Catalog, and job orchestration with Lakeflow Jobs.

You can read my full reflection on the summit and how each session impacted Kardiaflowâ€™s architecture here:  

ğŸ‘‰ [`docs/summit_reflections.md`](docs/summit_reflections.md)
